---
title: "MATH5905 - Assignment 1"
author: "Robert Clifford (z5058692)"
date: "16 August 2016"
output: pdf_document
---

```{r setup, include=FALSE}
require(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
```

*I declare that all work on this assignment is my own, unless acknolwedged. I have read and understand the University Rules in respect to Student Academic Misconduct.*

1. Consider a decision problem with parameters space $\Theta =\{\theta_1,\theta_2\}$ and a set of non randomised decisions $D=\{d_i,1\leq i \leq 6\}$ with risk points $\{R(\theta_1,d_i),R(\theta_2,d_i)\}$ as follows:



      | i | 1 | 2 | 3 | 4 | 5 | 6 |
      | --- | --- | --- | --- | --- | --- | --- | 
      | $R(\theta_1,d_i)$ | 0 | 1 | 2 | 5 | 6 | 4 |
      | $R(\theta_2,d_i)$ | 6 | 4 | 4 | 2 | 5 | 7 |
      

      a) Find the minimax rule(s) amongst the **nonrandomized** rules in D;
      
        The minimax rules are the set $\{d_2,d_3\}$ since the minimal between the 6 maximum values is $\{6,4,4,5,6,7\}$.
      b) Plot the risk set of all **randomised** rules $\mathcal{D}$ generated by the set of rules in D.
  
      
        ```{r bplot, echo=FALSE,out.width ="90%"}
        x <- data.frame(x = 0:8, y = 0:8)
        positions <- data.frame(
          labels = paste("d",1:6,sep=""),
          x = c(0,1,2,5,6,4),
          y = c(6,4,4,2,5,7)
        )
        labels <- data.frame(
          labels = paste("d",1:6,sep=""),
          x = positions$x+0.2,
          y = positions$y+0.2
        )
        g <- ggplot(positions[-3,], aes(x=x, y=y)) + 
          theme_bw() +
          xlim(0,8) + 
          ylim(0,8) + 
          geom_polygon(alpha = 0.5) +
          geom_line(data=positions[c(1,2,4),],aes(x=x,y=y), size = 1.2) +
          geom_line(data=x,aes(x=x,y=y),col="red") +
          geom_text(data=labels,aes(x=x,y=y,label=labels),check_overlap = TRUE) +
          geom_point(data=positions,aes(x=x,y=y)) +
          geom_point(data=data.frame(x=3,y=3),aes(x=x,y=y),colour="blue",shape = 8,size = 8) +
          geom_text(data=data.frame(x=3.8,y=3,labels="minimax"),aes(x=x,y=y,label=labels),check_overlap=TRUE) +
          annotate('text', x = 7.2, y = 6, label = "R( theta[2],d) == R(theta[1],d)",parse=TRUE) +
          labs( y = expression( R( theta[2],d)),
                x = expression( R( theta[1],d)))
          g
      ```  
      
      
        The risk set of all randomised rules generated by the rules in $D$ is given by the shaded area generated via the convex, $S$.
        The points of the decision rules show the non-randomised decision rules, while the admissable decision rules are given along the shaded lower left boundary of points $\{d_1,d_2,d_4\}$.
        
      c) Find the risk point of the minimax rule in $\mathcal{D}$ and determine its minimax risk.
      
        Using points $(1,4), (5,2)$, the slope of the curve is $m=\frac{4-2}{1-5}=-\frac{1}{2}$. We can then use $(1,4)$ to find the intercept, $R_2=-0.5R_1+c \to c = 4.5$. Therefore we have to solve for the equations $R_1 = R_2$ and $R_2 = -0.5R_1 + 4.5$ to find the intersect. 
        $$R_1=-0.5R_1+4.5 \to R_1 = 3$$
        
        Therefore the minimax risk is equal to $3$.
        
      d) Define the minimax rule in the set $\mathcal{D}$ in terms of rules in D.
      
          The minimax rule is a randomised rule of the form $d^*=\alpha d_2 + (1-\alpha)d_4$, such that $R(\theta_1,d^*) = R(\theta_2,d^*)$. This point is marked in $S$, and represents the randomised decision rule.
      
          We can now find alpha, to express the minimax rule in terms of the rules in D:
          $$ 
            \begin{aligned}
              \alpha d_2 + (1-\alpha)d_4 =3 \\ \text{Solving for } (1,4) \\
              \alpha \cdot 1 + (1-\alpha)\cdot 4 = 3 \\
              \alpha = \frac{1}{3}
            \end{aligned}
          $$
          
          So we choose $d_2 \text{with probablity} \frac{1}{3}$
          And we choose $d_4 \text{with probablity} \frac{2}{3}$
      
      e) For which prior on $\{\theta_1,\theta_2\}$ is the minimax rule a Bayes rule?
      
        The minimax rule occurs along the line $R_2 = -\frac{1}{2}R_1+c$. Given the Bayes rule can be found along $\alpha R_1 + (1-\alpha) R_2 = c$, we need to set $\alpha$ such that:
        
        $$\frac{\alpha}{1-\alpha} = \frac{1}{2} \to \alpha = \frac{1}{3}$$. 
        
        Then the prior is $\{ \theta_1, \theta_2\} = \{ \frac{1}{3}, \frac{2}{3}\}$.
      
        $$
        \begin{aligned}
        \frac{1}{3}R_1 + \frac{2}{3}R_2=c \\
          R_2 = \frac{1}{2}R_1 + \frac{3}{2}c
        \end{aligned}
        $$
        
        With these priors and $c = 4.5$ the Bayes rule will include the minimax rule.
      
        ```{r bayeMini, echo = FALSE,out.width ="90%"}
        g + 
         geom_abline(slope = -1/2, intercept = 3, linetype = 2) +
         geom_abline(slope = -1/2, intercept = 4.5, linetype = 2)
        ```
      
      
      f) Determine the Bayes rule and the Bayes risk for the prior $\left(\frac{4}{5},\frac{1}{5}\right)$ on $\{\theta_1,\theta_2\}$.
      
        The Bayes rule is determined by the straight line $\pi_1 R_1 + \pi_2 R_2 = c$ representing a class of decision rules with the same Bayes Risk. By varying c, we get the family of straight lines, choosing the one that intersects $S$.
        In this case, $c = 6$
        
        $$\begin{aligned}
          \frac{4}{5}R_1 + \frac{1}{5} R_2 = c \\
          \text{Solving for the point (0,6)} \\
          \frac{1}{5}\cdot 6 = c \to c = \frac{6}{5}
        \end{aligned}
        $$
        
        So the optimal line is:
        
        $$\begin{aligned}
         \frac{4}{5}R_1 + \frac{1}{5} R_2 = \frac{6}{5} \\
         R_2 = -4R_2 + 6
        \end{aligned}
        $$
        
        This intersects the admissable rules with $d_1$, so the bayes risk given the prior is:
        $$\frac{4}{5}R(\theta_1,d_1)+\frac{1}{5}R(\theta_2,d_1) = 1.2$$
        
        ```{r bayesRisk, echo = FALSE,out.width ="90%"}
        g + 
         geom_abline(slope = -4, intercept = 3, linetype = 2) +
         geom_abline(slope = -4, intercept = 4.5, linetype = 2) + 
         geom_abline(slope = -4, intercept = 6, linetype = 2)
        ```
      
      g) For $\epsilon=\frac{4}{5}$, illustrate on the risk set the risk points of all rules which are $\epsilon$-minimax.
      
        The $\epsilon$-minimax decision rule is equal to
        
        $$
          \sup_{\theta\in\Theta}R(\theta,\delta_\epsilon)\leq\inf_{\delta\in\mathcal{D}}\sup_{\theta\in\Theta}R(\theta,\delta)+\epsilon
        $$
        
        The $\delta_\epsilon$ rules are those those that are:
        
        $$\sup_{\theta\in\Theta}R(\theta,\delta_\epsilon)\leq 3 + \frac{4}{5}
        $$
        
        These include $\{d_2,d_3\}$
        
        ```{r epsilon, echo = FALSE,out.width ="90%"}
        g + 
         geom_polygon(data=data.frame(
                        x=c(1.4,3.8,3.8),y=c(3.8,2.6,3.8)),aes(x=x,y=y),colour = "blue",alpha=0.5)
        ```
    
2. In a sequence of consecutive years $1,2,...,T$ an annual number of high-risk events is recorded by a bank. The random number $N_t$ of high-risk events in a given year is modelled via Poisson($\lambda$) distribution. This gives a sequence of independent counts $n_1,n_2,...,n_T$. The prior on $\lambda$ is Gamma(a,b) with known $a>0,b>0$:

      $$\tau(\lambda)=\frac{\lambda^{a-1}e^{-\lambda/b}}{\Gamma(a)b^a},\lambda>0.$$

      a) Determine the Bayesian estimator of the intensity $\lambda$ with respect to quadratic loss.
      
        The distribution of the $T$ events is given by:
        $$
        \begin{aligned}
          f(x_1,...,x_n|\lambda)&=\frac{\lambda^{\Sigma_{i=1}^n x_i}e^{-\lambda n}}{\prod_{i=1}^{n} x_i !} \\
          f(x_1,...,x_n,\lambda)&=f(X|\lambda)\tau(\lambda) \\
            &=\frac{\lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)}}{\prod_{i=1}^{n} x_i !\Gamma(a)b^a} \\
          h(x_1,...,x_n) &=\int_0^\infty f(x_1,...,x_n,\lambda)\partial \lambda \\
          &=\int_0^\infty \frac{\lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)}}{\prod_{i=1}^{n} x_i !\Gamma(a)b^a} \partial \lambda \\
          &=\frac{1}{\prod_{i=1}^{n} x_i !\Gamma(a)b^a} \int_0^\infty \lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)} \partial \lambda \\
        \end{aligned}
        $$
        
        Noticing that the integral is a gamma distribution, we can multiple by a constant and, and integrating over the entire distribution is equal to $1$.
        
        $$
          h(x_1,...,x_n) = \frac{\Gamma(\Sigma_{i=1}^n x_i + a)}
                    {\prod_{i=1}^{n} x_i !\Gamma(a)b^a(n+1/b)^{\Sigma_{i=1}^n x_i + a}}
        $$
        
        The conditional prior distribtuion is therefore:
        
        $$\begin{aligned}
          \tau(\lambda|x_1,...,x_n) &= \frac{f(x_1,...,x_n,\lambda)}{h(x_1,...,x_n)} \\
          &={\frac{\lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)}}{\prod_{i=1}^{n} x_i !\Gamma(a)b^a}}/{\frac{\Gamma(\Sigma_{i=1}^n x_i + a)}
                    {\prod_{i=1}^{n} x_i !\Gamma(a)b^a(n+1/b)^{\Sigma_{i=1}^n x_i + a}}} \\
                    &=\frac{(n+1/b)^{\Sigma_{i=1}^n x_i + a}}{\Gamma(\Sigma_{i=1}^n x_i + a)}\lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)} \sim \text{Gamma}(a+\Sigma x_i, n+1/b)
          \end{aligned}
        $$
        
        The Bayes estimator with respect to quadratic loss is equal to the expected value of the conditional prior distribtion $E(\lambda) = \int_\Lambda \lambda \tau(\lambda|X)\partial\lambda$. For a Gamma distribution, the mean is given by $\frac{\alpha}{\beta}$. Therefore;
        
        $$\hat{\lambda}_{\text{Bayes}}=\frac{a+\Sigma x_i}{n+1/b}$$
      
      b) Assume $a = 3, b =2$. Within the last seven years counts were $2,4,7,3,4,4,5$. Find the Baye's estimate of the intensity $\lambda$ using quadratic loss and the above data.
      
      $$\hat{\lambda}_{\text{Bayes}}=\frac{3+29}{7+1/2}=\frac{64}{15}\approx 4.267$$
      
      
      c) The bank claims that the yearly intensity $\lambda$ is less than 4. Test the bank's claim via Bayesian testing with a zero-one loss.
      
        The null hypothesis is:
        $$
        \begin{aligned}
          H_0 &: \lambda < 4 \\
          H_1 &: \lambda > 4
        \end{aligned}
        $$
        
        Testing the hypothesis:
        $$
        \begin{aligned}
          \tau(\lambda \in \Lambda_0 | X) &=\int_0^4 \frac{(n+1/b)^{\Sigma_{i=1}^n x_i + a}}{\Gamma(\Sigma_{i=1}^n x_i + a)}\lambda^{\Sigma_{i=1}^n x_i + a - 1}e^{-\lambda(n+1/b)} \partial \lambda \\
          &=\int_0^4 \frac{(7+1/2)^{29 + 3}}{\Gamma(29 + 3)}\lambda^{29 + 3 - 1}e^{-\lambda(7+1/2)} \partial \lambda
        \end{aligned}
        $$
  
            ```{r}
            fn <- function(x) ((7.5)^32*x^(31)*exp(-x*(7.5)))/(gamma(32));
            integrate(fn,0,4)
            ```
          
          The test is rejected if $H_0<\frac{1}{2}$, so we reject the null hypothesis that the intensity $\lambda$ is less than 4 and accept the alternative that $\lambda > 4$.
        
    
3. Let $X_1,X_2,...,X_n$ be i.i.d. uniform in $(0,\theta)$ and let the prior on $\theta$ be the Pareto prior given by $\tau(\theta) = \beta\alpha^\beta\theta^{-(\beta+1)},\theta>\alpha$. (Here $\alpha > 0$ and $\beta >0$ are assumed to be known). Show that the Bayes estimator with respect to quadratic loss is given by $\hat{\theta}=\max(\alpha,x_{(n)})\frac{n+\beta}{n+\beta-1}$. Justify all steps in derivation.


    $$f(x_1,...,x_n|\theta)=\theta^{-n}, \theta>x_{(n)}$$
    $$\tau(\theta) = \beta \alpha^\beta\theta^{-(\beta+1)}, \theta>\alpha$$
    
    
    We can calculate the conditional prior distribution:
    
    
    $$\begin{aligned}
        f(x_1,...,x_n,\theta) &=\beta \alpha^\beta\theta^{-(\beta+n+1)}, \theta>x_{(n)},\alpha \\
        h(x_1,...,x_n) &= \int_{x_{(n)}}^\infty \beta \alpha^\beta\theta^{-(\beta+n+1)} \partial \theta \\
        &=\begin{cases}
            \frac{\beta\alpha^\beta}{(n+\beta)\alpha^{n+\beta}}=\frac{\beta}{(n+\beta)\alpha^{n}}, & \text{if}\ \alpha > x_{(n)} \\
            \frac{\beta\alpha^\beta}{(n+\beta)x_{(n)}^{n+\beta}}, & \text{if}\ \alpha < x_{(n)} \\
          \end{cases} \\
        \tau(\theta|x_1,...,x_n) &=\begin{cases}
            \frac{\beta\alpha^\beta\theta^{-(\beta+n+1)}}{\frac{\beta}{(n+\beta)\alpha^{n}}}
            =\alpha^{n+\beta}(n+\beta)\theta^{-(\beta+n+1)}, & \text{if}\ \alpha > x_{(n)} \\
            \frac{\beta\alpha^\beta\theta^{-(\beta+n+1)}}{\frac{\beta\alpha^\beta}{(n+\beta) x_{(n)}^{n+\beta}}} =x_{(n)}^{n+\beta}(n+\beta)\theta^{-(\beta+n+1)}, & \text{if}\ \alpha > x_{(n)}
          \end{cases} \\
        &=\text{max}(\alpha,x_{(n)})^{n+\beta}(n+\beta)\theta^{-(\beta + n + 1)} \sim \text{Pareto}(\text{max}(\alpha,x_{(n)}), n+\beta)
      \end{aligned}
    $$
    
    The Bayes estimator with respect to quadratic loss is equal to the expected value of the conditional prior distribtion $E(\theta) = \int_\Theta \theta \tau(\theta|X)\partial\theta$. For a Pareto distribution, the mean is given by $\frac{\alpha\beta}{\alpha-1}$. Therefore;
    
    $$\hat{\theta}=\max(\alpha,x_{(n)})\frac{n+\beta}{n+\beta-1}$$

4. At a critical stage in the development of a new aeroplane in the UK (jointly contracted with the French), a decision must be taken to continue or to abandon the project. The financial viability of the project can be measured by a parameter $\theta \in (0,1)$, the project being profitable if $\theta > \frac{1}{2}$, the cost to the taxpayer of continuing the project is $(\frac{1}{2}-\theta)$(in units of \$ billion) whereas if $\theta > \frac{1}{2}$, it is zero (since the project will be privatised if profitable). Data $x$ provides information about $\theta$ : the prototype aeroplane is subjected to trials, each independently having probabliity $\theta$ of success, and the data $x$ consists of the total number of trials requried for the first successful result to be obtained (in other wrods, we observe a single realisation of a geometrically distributed random variable). If $\theta > \frac{1}{2}$ the cost of abandoning the project is $(\theta - \frac{1}{2})$(due to contractual arrangements for purchasing the aeroplane from the French), whereas if $\theta < \frac{1}{2}$ it is zero. Two actions are on the table: $a_0$ : continue the project and $a_1$ : abandon it.


    i) Derive the Bayesian decision rule, for a given prior on $\theta$. In particular, show that the rule is equivalent to comparing the posterior mean of $\theta$ given $x$ with a threshold constant.
    
        There are two actions:
        $a_0$: continue the project
        $a_1$: abandon the project.
        
        The cost of continueing: $L(\theta,a_0)=\begin{cases}
                                    0, & \text{if}\ \theta > \frac{1}{2} \\
                                    \frac{1}{2} - \theta, & \text{if}\ \theta < \frac{1}{2}
                                  \end{cases}$
        
        The cost of abandoning: $L(\theta,a_1)=\begin{cases}
                                    \theta - \frac{1}{2}, & \text{if}\ \theta > \frac{1}{2} \\
                                    0, & \text{if}\ \theta < \frac{1}{2}
                                  \end{cases}$
        
                                  
        Project will be continued if the cost is less than abandoning.
        
        $$
        \begin{aligned}
        E_{\theta}  \left(L( \theta,a_0)\right) &> E_{\theta}\left(L(\theta,a_1)\right) \\
          \int_0^{\frac{1}{2}}\left( \frac{1}{2}-\theta \right) \tau \left( \theta|x \right) \partial \theta &>     \int_{\frac{1}{2}}^1 \left( \theta-\frac{1}{2} \right) \tau \left( \theta|x \right) \partial \theta
        \end{aligned}
        $$
        $$
          \begin{aligned}
          0 &> \int_{0}^1 \left( \theta-\frac{1}{2} - \frac{1}{2} + \theta \right) \tau \left( \theta|x \right) \partial \theta \\
          0 &> \int_{0}^1 \left( \theta-\frac{1}{2} \right) \tau \left( \theta|x \right) \partial \theta \\
          0 &> \int_{0}^1 \left( \theta \right) \tau \left( \theta|x \right) \partial \theta - \frac{1}{2} \\
          \frac{1}{2} &> \int_{0}^1 \left( \theta \right) \tau \left( \theta|x \right) \partial \theta = \mu\left(\theta\vert x\right)
          \end{aligned}
        $$
      
    
    ii) The Minister of Aviation has prior density $6\theta(1-\theta)$ for $\theta$. The prime minister has prior density $4\theta^3$. For which values of $x$ will there be most serious ministerial disagreement?
    
        There will be disagreement when one Minister wants to abandon the project, while the other wants to continue it.
    
        The conditional distribution on $x$ is
        $$f\left( x;\theta \right)=\theta \left(1-\theta\right)^{x-1}, x=1,2,...$$
      
        Both the Minister for Aviation and the Prime Minster have beta prior distributions.  
        For the Minister for Aviation (MA):
        $$\tau_{MA}\left(\theta\right) = 6\theta(1-\theta) \sim Beta(2,2)$$
        
        So calculating the conditonal prior distribution:
        $$\begin{aligned}
            f(x,\theta) &= 6\theta^2(1-\theta)^{x} \\
            h(x) &=\int_0^1 6\theta^2(1-\theta)^{x} \partial \theta \\
                & = 6\cdot \text{beta}(3,x+1) \\
            \tau(\theta|x) &=\frac{6\theta^2(1-\theta)^{x}}{ 6\cdot \text{beta}(3,x+1)} \\
              & = \frac{1}{\text{beta}(3,x+1)}\theta^2(1-\theta)^{x} \sim \text{Beta}(3,x+1)
          \end{aligned}
        $$
        
        The prior distribtuion for the Prime Minister (PM):
        $$\tau_{PM}\left(\theta\right) = 4\theta^3 \sim Beta(5,1)$$
        
        So calculating the conditional prior distribution:
        $$\begin{aligned}
            f(x,\theta) &= 4\theta^4(1-\theta)^{x-1} \\
            h(x) &=\int_0^1 4\theta^4(1-\theta)^{x-1.} \partial \theta \\
                & = 4\cdot \text{beta}(5,x) \\
            \tau(\theta|x) &=\frac{4\theta^4(1-\theta)^{x-1}}{ 4\cdot \text{beta}(5,x)} \\
              & = \frac{1}{\text{beta}(5,x)}\theta^4(1-\theta)^{x-1} \sim \text{Beta}(5,x)
          \end{aligned}
        $$
        
        If $x \sim \text{Beta}(a,b)$ then $E(x)=\frac{a}{a+b}$.
        
        The Minister of Aviation will want to abandon if  $\frac{3}{4 +x}<\frac{1}{2}$, ie $x>2$ with $x=2$ being on the edge. The Prime Minster will want to abandon if $\frac{5}{5+x}<\frac{1}{2}$, ie $x>5$ with $x=5$ being on the edge. Therefore there will be the greatest amount of disagreement when $x \in \{ 3,4 \}$.
      